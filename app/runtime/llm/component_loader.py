from __future__ import annotations

import logging
from typing import Any

from tqdm.auto import tqdm  # noqa: E402
from transformers import AutoModel, AutoProcessor, AutoTokenizer

from app.runtime.llm.model_importer import resolve_pretrained_source
from app.runtime.llm.model_manager import torch_dtype_for_device

logger = logging.getLogger(__name__)


def _download_with_progress(pretrained_source: str, cache_dir: str | None = None, desc: str = "ä¸‹è½½æ¨¡å‹"):
    """ä½¿ç”¨è¿›åº¦æ¡ä¸‹è½½ HuggingFace æ¨¡å‹"""
    try:
        from huggingface_hub import HfApi, snapshot_download

        tqdm.write(f"ğŸ“¦ æ­£åœ¨ä¸‹è½½ {desc}...")

        api = HfApi()

        repo_info = api.repo_info(pretrained_source, repo_type="model")
        siblings = getattr(repo_info, 'siblings', [])
        if not siblings:
            siblings = getattr(repo_info, 'files', [])

        total_files = len(siblings)
        if total_files == 0:
            snapshot_download(pretrained_source, cache_dir=cache_dir)
            return

        with tqdm(total=total_files, desc=f"ä¸‹è½½ {desc}", unit="æ–‡ä»¶") as pbar:
            for sibling in siblings:
                filename = sibling.rfilename if hasattr(sibling, 'rfilename') else sibling
                try:
                    api.hf_hub_download(
                        filename=filename,
                        repo_id=pretrained_source,
                        repo_type="model",
                        cache_dir=cache_dir,
                        resume_download=True,
                    )
                except Exception as e:
                    logger.debug(f"Failed to download {filename}: {e}")
                pbar.update(1)
    except Exception as e:
        logger.warning(f"Model download failed: {e}")


def resolve_pretrained_source_for_spec(spec: Any) -> str:
    """
    æ ¹æ® ModelSpec è§£æé¢„è®­ç»ƒæ¨¡å‹æºè·¯å¾„ã€‚
    è‡ªåŠ¨å¤„ç† ModelScope/HuggingFace çš„ä¸‹è½½é€»è¾‘ã€‚
    """
    imported = resolve_pretrained_source(
        provider=spec.provider,
        model_ref=spec.model_ref,
        cache_dir=spec.cache_dir,
        revision=spec.revision,
        modelscope_fallback_to_hf=spec.modelscope_fallback_to_hf,
    )
    return imported.pretrained_source


def load_transformers_model(
    pretrained_source: str,
    *,
    trust_remote_code: bool,
    device: str,
    model_type: str = "auto",
    model_name: str = "æ¨¡å‹",
) -> Any:
    """
    åŠ è½½ Transformers æ¨¡å‹ã€‚
    æ”¯æŒè‡ªåŠ¨æ¨¡å‹ (AutoModel) å’Œåºåˆ—åˆ†ç±»æ¨¡å‹ (AutoModelForSequenceClassification)ã€‚
    æ˜¾ç¤ºä¸‹è½½è¿›åº¦æ¡ã€‚
    """
    import tempfile

    from tqdm.auto import tqdm

    cache_dir = tempfile.gettempdir()
    tqdm.write(f"ğŸ“¦ æ­£åœ¨ä¸‹è½½ {model_name}...")

    _download_with_progress(pretrained_source, cache_dir=cache_dir, desc=f"ä¸‹è½½ {model_name}")

    if model_type == "sequence_classification":
        from transformers import AutoModelForSequenceClassification

        model = AutoModelForSequenceClassification.from_pretrained(
            pretrained_source,
            trust_remote_code=trust_remote_code,
            torch_dtype=torch_dtype_for_device(device),
        )
    else:
        model = AutoModel.from_pretrained(
            pretrained_source,
            trust_remote_code=trust_remote_code,
            torch_dtype=torch_dtype_for_device(device),
        )
    model = model.to(device)
    model.eval()
    return model


def try_load_transformers_processor(pretrained_source: str, *, trust_remote_code: bool) -> Any | None:
    """å°è¯•åŠ è½½ Transformers Processorï¼Œå¤±è´¥è¿”å› None"""
    try:
        return AutoProcessor.from_pretrained(pretrained_source, trust_remote_code=trust_remote_code)
    except Exception as e:
        logger.debug(f"Failed to load processor for {pretrained_source}: {e}")
        return None


def load_transformers_tokenizer(pretrained_source: str, *, trust_remote_code: bool) -> Any:
    """åŠ è½½ Transformers Tokenizer"""
    return AutoTokenizer.from_pretrained(pretrained_source, trust_remote_code=trust_remote_code)


def load_sentence_transformers_embedder(
    pretrained_source: str,
    *,
    device: str,
    max_length: int | None = None,
    model_name: str = "åµŒå…¥æ¨¡å‹",
) -> Any:
    """åŠ è½½ SentenceTransformer åµŒå…¥æ¨¡å‹ï¼Œå¸¦ä¸‹è½½è¿›åº¦æ¡"""
    import tempfile

    from sentence_transformers import SentenceTransformer

    cache_dir = tempfile.gettempdir()
    tqdm.write(f"ğŸ“¦ æ­£åœ¨ä¸‹è½½ {model_name}...")

    _download_with_progress(pretrained_source, cache_dir=cache_dir, desc=f"ä¸‹è½½ {model_name}")

    model = SentenceTransformer(pretrained_source, device=device)
    if max_length is not None:
        try:
            model.max_seq_length = int(max_length)
        except (ValueError, TypeError) as e:
            logger.warning(f"Failed to set max_seq_length to {max_length}: {e}")
    return model


def load_sentence_transformers_cross_encoder(
    pretrained_source: str,
    *,
    device: str,
    max_length: int | None = None,
    model_name: str = "é‡æ’åºæ¨¡å‹",
) -> Any:
    """åŠ è½½ SentenceTransformer CrossEncoder æ¨¡å‹ï¼Œå¸¦ä¸‹è½½è¿›åº¦æ¡"""
    import tempfile

    from sentence_transformers import CrossEncoder

    cache_dir = tempfile.gettempdir()
    tqdm.write(f"ğŸ“¦ æ­£åœ¨ä¸‹è½½ {model_name}...")

    _download_with_progress(pretrained_source, cache_dir=cache_dir, desc=f"ä¸‹è½½ {model_name}")

    return CrossEncoder(pretrained_source, device=device, max_length=max_length)

