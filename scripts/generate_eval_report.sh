#!/bin/zsh

# AgFrame Evaluation Report Generator
# ç”Ÿæˆæµ‹è¯•è¯„ä¼°æŠ¥å‘Š

set -e

SCRIPT_DIR="$(cd "$(dirname "${0}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
REPORT_DIR="$PROJECT_ROOT/reports"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

mkdir -p "$REPORT_DIR"

echo "========================================"
echo "ğŸ“Š AgFrame Evaluation Report Generator"
echo "========================================"

cd "$PROJECT_ROOT"

# æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ
if [[ -d ".venv" ]]; then
    source .venv/bin/activate
fi

# è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆ JSON æŠ¥å‘Š
echo "Running tests with JSON output..."
python -m pytest tests/ \
    -v \
    --tb=short \
    --json-report \
    --json-report-file="$REPORT_DIR/test_report_$TIMESTAMP.json" \
    || true

# è¿è¡Œ DeepEval è¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
echo ""
echo "Running DeepEval metrics..."
python -c "
import sys
sys.path.insert(0, '$PROJECT_ROOT')
try:
    from deepeval import run_test_cases
    from deepeval.test_case import LLMTestCase
    from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric

    import json
    with open('$PROJECT_ROOT/tests/fixtures/golden_cases.json') as f:
        data = json.load(f)

    test_cases = []
    for case in data['cases']:
        tc = LLMTestCase(
            input=case['input'],
            actual_output=f'é’ˆå¯¹ \"{case[\"input\"]}\" çš„å›ç­”',
            retrieval_context=['ç›¸å…³æ–‡æ¡£'],
        )
        test_cases.append(tc)

    answer_relevancy = AnswerRelevancyMetric()
    faithfulness = FaithfulnessMetric()

    for tc in test_cases:
        ar_score = answer_relevancy.measure(tc)
        f_score = faithfulness.measure(tc)
        print(f'{case[\"id\"]}: AnswerRelevancy={ar_score:.3f}, Faithfulness={f_score:.3f}')

    print('\\nâœ… DeepEval metrics complete')
except ImportError:
    print('âš ï¸  DeepEval not installed, skipping')
except Exception as e:
    print(f'âš ï¸  DeepEval error: {e}')
"

# ç”Ÿæˆ Markdown æŠ¥å‘Š
echo ""
echo "Generating markdown report..."
cat > "$REPORT_DIR/eval_report_$TIMESTAMP.md" << EOF
# AgFrame Evaluation Report

**ç”Ÿæˆæ—¶é—´**: $(date)

## æµ‹è¯•æ‘˜è¦

| æŒ‡æ ‡ | å€¼ |
|------|-----|
| æµ‹è¯•ç”¨ä¾‹æ•° | $(python -c "import json; print(len(json.load(open('$REPORT_DIR/test_report_$TIMESTAMP.json', errors='ignore').read())['tests']))" 2>/dev/null || echo "N/A") |
| é€šè¿‡ | $(python -c "import json; d=json.load(open('$REPORT_DIR/test_report_$TIMESTAMP.json', errors='ignore').read()); print(d.get('summary',{}).get('passed', 'N/A'))" 2>/dev/null || echo "N/A") |
| å¤±è´¥ | $(python -c "import json; d=json.load(open('$REPORT_DIR/test_report_$TIMESTAMP.json', errors='ignore').read()); print(d.get('summary',{}).get('failed', 'N/A'))" 2>/dev/null || echo "N/A") |

## Golden Dataset è¯„ä¼°

| Case ID | è¾“å…¥ | é¢„æœŸå·¥å…· | Answer Relevancy | Faithfulness |
|---------|------|----------|------------------|--------------|
EOF

# æ·»åŠ  Golden Dataset è¯„ä¼°ç»“æœ
python -c "
import json
with open('$PROJECT_ROOT/tests/fixtures/golden_cases.json') as f:
    data = json.load(f)
for case in data['cases']:
    print(f'| {case[\"id\"]} | {case[\"input\"][:30]}... | {case[\"expected_tool\"]} | - | - |')
" >> "$REPORT_DIR/eval_report_$TIMESTAMP.md"

echo "" >> "$REPORT_DIR/eval_report_$TIMESTAMP.md"
echo "## å»ºè®®" >> "$REPORT_DIR/eval_report_$TIMESTAMP.md"
echo "" >> "$REPORT_DIR/eval_report_$TIMESTAMP.md"
echo "- æŒç»­ç›‘æ§ Answer Relevancy å’Œ Faithfulness æŒ‡æ ‡" >> "$REPORT_DIR/eval_report_$TIMESTAMP.md"
echo "- å½“æŒ‡æ ‡ä¸‹é™æ—¶ï¼Œæ£€æŸ¥æœ€è¿‘çš„ä»£ç å˜æ›´" >> "$REPORT_DIR/eval_report_$TIMESTAMP.md"
echo "- å®šæœŸæ›´æ–° Golden Dataset ä»¥è¦†ç›–æ–°åœºæ™¯" >> "$REPORT_DIR/eval_report_$TIMESTAMP.md"

echo ""
echo "âœ… æŠ¥å‘Šå·²ç”Ÿæˆ:"
echo "   - JSON: $REPORT_DIR/test_report_$TIMESTAMP.json"
echo "   - Markdown: $REPORT_DIR/eval_report_$TIMESTAMP.md"
